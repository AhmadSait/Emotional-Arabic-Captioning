{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb18be4b-59eb-423f-a3f2-778915b64ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Requirement already satisfied: transformers in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (4.51.2)\n",
      "Requirement already satisfied: filelock in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: blobfile in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from blobfile) (3.22.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from blobfile) (2.4.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from blobfile) (5.3.2)\n",
      "Requirement already satisfied: filelock>=3.0 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from blobfile) (3.13.1)\n",
      "Requirement already satisfied: sentencepiece in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken \n",
    "!pip install transformers\n",
    "!pip install blobfile\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef7226b-98ea-4cf8-8752-e400ff1d6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: protobuf in /home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages (6.30.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49e9646-a266-4e9c-8f34-0c4aa7afb6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e83ae2f7ee346c6b9c771c01c18cd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\ntotal_params = sum(p.numel() for p in hunayn_model.parameters())\\ntrainable_params = sum(p.numel() for p in hunayn_model.parameters() if p.requires_grad)\\n\\nprint(f\"Total parameters: {total_params:,}\")\\nprint(f\"Trainable parameters: {trainable_params:,}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "allam_model = AutoModelForCausalLM.from_pretrained(\"ALLaM-AI/ALLaM-7B-Instruct-preview\").to(device)\n",
    "tokenizer_allam = AutoTokenizer.from_pretrained(\"ALLaM-AI/ALLaM-7B-Instruct-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38cf582c-4d48-43bc-960e-f223683da6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hunayn_model = MarianMTModel.from_pretrained(\"Hunayn/Big_Hunayn_at_different_epochs/model_at_epoch7\").to(device)\n",
    "hunayn_tokenizer = MarianTokenizer.from_pretrained(\"Hunayn/Big_Hunayn_at_different_epochs/model_at_epoch7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f830bb21-fb8f-4644-a0a1-3197487f8895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/saitaa0b/miniconda3/envs/allam_hunayn/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 50/50 [01:39<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to arabic_enhancement_comparison.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# reading first 100 from artelingo\n",
    "file_path = \"artelingo_release.csv\"\n",
    "df_arabic = pd.read_csv(\n",
    "    file_path, \n",
    "    skiprows=range(1, 386414), \n",
    "    nrows=50, \n",
    "    usecols=[\"utterance_spelled\", \"image_file\"],\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#extracting the artelingo Arabic captions and making three new columns\n",
    "arabic_column = \"utterance_spelled\"\n",
    "df_arabic = df_arabic.rename(columns={\"utterance_spelled\": \"artelingo_arabic\"})\n",
    "df_arabic[\"allam_arabic\"] = \"\"\n",
    "df_arabic[\"translated_english\"] = \"\"\n",
    "df_arabic[\"hunayn_arabic\"] = \"\"\n",
    "\n",
    "def clean_allam_response(text, system_prompt, user_input):\n",
    "    # sometimes special token are outputted so I'm removing them here\n",
    "    text = re.sub(r\"\\[INST\\]|\\[/INST\\]|<<SYS>>|<</SYS>>\", \"\", text).strip()\n",
    "\n",
    "    # someitmes the system prompt itself is being outputted so I'm deleting it\n",
    "    if system_prompt in text:\n",
    "        text = text.split(system_prompt, 1)[-1].strip()\n",
    "\n",
    "    # someimes the user prompt is being outputted too so I'm taking deleting it\n",
    "    if user_input in text:\n",
    "        text = text.split(user_input, 1)[-1].strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def enhance_arabic_allam(text):\n",
    "    system_msg = \"خذ الجملة المعطاة وأعد صياغتها باللغة العربية الفصحى بأسلوبٍ أكثر فصاحةً ورقيًّا، مع الحفاظ على نفس المعنى\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    input_text = tokenizer_allam.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer_allam(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = allam_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.6,\n",
    "            pad_token_id=tokenizer_allam.eos_token_id\n",
    "        )\n",
    "    return clean_allam_response(tokenizer_allam.decode(output[0], skip_special_tokens=True), system_msg, text)\n",
    "\n",
    "def translate_arabic_to_english_allam(text):\n",
    "    system_msg = \"Your job is to strictly only translate the following Arabic sentence to English.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    input_text = tokenizer_allam.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer_allam(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = allam_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.6,\n",
    "            pad_token_id=tokenizer_allam.eos_token_id\n",
    "        )\n",
    "    return clean_allam_response(tokenizer_allam.decode(output[0], skip_special_tokens=True), system_msg, text)\n",
    "\n",
    "def translate_english_to_arabic_hunayn(text):\n",
    "    input_ids = hunayn_tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = hunayn_model.generate(input_ids, max_length=len(text.split()) + 10, num_beams=5)\n",
    "    return hunayn_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# going through each row in the dataframe\n",
    "for idx, row in tqdm(df_arabic.iterrows(), total=len(df_arabic)):\n",
    "    modern = row[\"artelingo_arabic\"]\n",
    "\n",
    "    # enhance Arabic via ALLaM\n",
    "    enhanced = enhance_arabic_allam(modern)\n",
    "    df_arabic.at[idx, \"allam_arabic\"] = enhanced\n",
    "\n",
    "    # translate Arabic to English via ALLaM\n",
    "    english = translate_arabic_to_english_allam(modern)\n",
    "    df_arabic.at[idx, \"translated_english\"] = english\n",
    "\n",
    "    # translate English back to Enhanced Arabic via Hunayn\n",
    "    back_arabic = translate_english_to_arabic_hunayn(english)\n",
    "    df_arabic.at[idx, \"hunayn_arabic\"] = back_arabic\n",
    "\n",
    "output_file = \"arabic_enhancement_comparison.csv\"\n",
    "df_arabic.to_csv(output_file, encoding=\"utf-8-sig\", index=False)\n",
    "print(f\"Saved results to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de15b476-5fe7-47c2-b7a0-c95ddd3e03ee",
   "metadata": {},
   "source": [
    "Since ALLAM is the preferred the model over HUNAYN, only ALLAM enhanced will be outputted now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949b43f9-4c4f-4267-b479-658852f8adbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhancing Batches:   0%|          | 0/7729 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved results to allam_enhancements_only.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "file_path = \"artelingo_release.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer_allam.pad_token = tokenizer_allam.eos_token\n",
    "tokenizer_allam.padding_side = \"left\"\n",
    "\n",
    "df_arabic = pd.read_csv(\n",
    "    file_path, \n",
    "    skiprows=range(1, 386414), \n",
    "    nrows=386410, \n",
    "    usecols=[\"utterance_spelled\", \"image_file\"],\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "df_arabic = df_arabic.rename(columns={\"utterance_spelled\": \"artelingo_arabic\"})\n",
    "df_arabic[\"allam_arabic\"] = \"\"\n",
    "\n",
    "def clean_allam_response(text, system_prompt, user_input):\n",
    "    text = re.sub(r\"\\[INST\\]|\\[/INST\\]|<<SYS>>|<</SYS>>\", \"\", text).strip()\n",
    "    if system_prompt in text:\n",
    "        text = text.split(system_prompt, 1)[-1].strip()\n",
    "    if user_input in text:\n",
    "        text = text.split(user_input, 1)[-1].strip()\n",
    "    return text\n",
    "\n",
    "def enhance_batch_allam(batch_texts):\n",
    "    system_msg = \"خذ الجملة المعطاة وأعد صياغتها باللغة العربية الفصحى بأسلوبٍ أكثر فصاحةً ورقيًّا، مع الحفاظ على نفس المعنى\"\n",
    "    prompts = [\n",
    "        tokenizer_allam.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ], tokenize=False) for text in batch_texts\n",
    "    ]\n",
    "    inputs = tokenizer_allam(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = allam_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.6,\n",
    "            pad_token_id=tokenizer_allam.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer_allam.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return [clean_allam_response(decoded_output, system_msg, original_input) for decoded_output, original_input in zip(decoded, batch_texts)]\n",
    "\n",
    "batch_size = 50\n",
    "for start_idx in tqdm(range(0, len(df_arabic), batch_size), desc=\"Enhancing Batches\", leave=False):\n",
    "    end_idx = min(start_idx + batch_size, len(df_arabic))\n",
    "    batch = df_arabic.iloc[start_idx:end_idx]\n",
    "    modern_batch = batch[\"artelingo_arabic\"].tolist()\n",
    "    enhanced_batch = enhance_batch_allam(modern_batch)\n",
    "    df_arabic.loc[start_idx:end_idx - 1, \"allam_arabic\"] = enhanced_batch\n",
    "\n",
    "output_file = \"allam_enhancements_only.csv\"\n",
    "df_arabic = df_arabic[[\"image_file\", \"allam_arabic\"]]\n",
    "df_arabic.to_csv(output_file, encoding=\"utf-8-sig\", index=False)\n",
    "print(f\"✅ Saved results to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1f526-6f91-4250-b4cc-e26926707d52",
   "metadata": {},
   "source": [
    "This took 14 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d54dd4-4705-42e7-b28a-1ffa2e58db23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
